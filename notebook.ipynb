{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import time\n",
    "import neurogym as ngym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuroGym Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'PerceptualDecisionMaking-v0'\n",
    "kwargs = {'dt': 20, 'timing': {'stimulus': 1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (SeqLen, Batch, Dim) = torch.Size([100, 16, 3])\n",
      "Target has shape (SeqLen, Batch) = (100, 16)\n"
     ]
    }
   ],
   "source": [
    "# Make supervised dataset\n",
    "seq_len = 100\n",
    "batch_size = 16\n",
    "#Create the dataset (Hover over ngym.Dataset to see input arguments)\n",
    "dataset = ngym.Dataset(task_name, env_kwargs=kwargs, seq_len=seq_len, batch_size=batch_size)\n",
    "env = dataset.env\n",
    "\n",
    "# Generate one batch of data when called\n",
    "inputs, target = dataset()\n",
    "inputs = torch.from_numpy(inputs).type(torch.float)\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "print('Input has shape (SeqLen, Batch, Dim) =', inputs.shape)\n",
    "print('Target has shape (SeqLen, Batch) =', target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataset, criterion, model = None):\n",
    "    \"\"\"Simple helper function to train the model.\n",
    "\n",
    "    Args:\n",
    "        net: a pytorch nn.Module module\n",
    "        dataset: a dataset object that when called produce a (input, target output) pair\n",
    "        criterion: a pytorch loss function\n",
    "        model: a model object that is used in the loss function\n",
    "\n",
    "    Returns:\n",
    "        net: network object after training\n",
    "    \"\"\"\n",
    "    # Use Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    start_time = time.time()\n",
    "    # Loop over training batches\n",
    "    print('Training network...')\n",
    "    for i in range(2000):\n",
    "        # Generate input and target, convert to pytorch tensor\n",
    "        inputs, labels = dataset()\n",
    "        inputs = torch.from_numpy(inputs).type(torch.float)\n",
    "        labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
    "\n",
    "        # boiler plate pytorch training:\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        output, _ = net(inputs)\n",
    "        # Reshape to (SeqLen x Batch, OutputSize)\n",
    "        output = output.view(-1, output_size)\n",
    "        if model == None:\n",
    "            loss = criterion(output, labels)\n",
    "        else:\n",
    "            loss = criterion(output, labels, model)\n",
    "        loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "\n",
    "        # Compute the running loss every 100 steps\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            running_loss /= 100\n",
    "            print('Step {}, Loss {:0.4f}, Time {:0.1f}s'.format(\n",
    "                i+1, running_loss, time.time() - start_time))\n",
    "            running_loss = 0\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Leaky RNN (from notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyRNN(nn.Module):\n",
    "    \"\"\"Leaky RNN.\n",
    "\n",
    "    Parameters:\n",
    "        input_size: Number of input neurons\n",
    "        hidden_size: Number of hidden neurons\n",
    "        dt: discretization time step in ms.\n",
    "            If None, dt equals time constant tau\n",
    "\n",
    "    Inputs:\n",
    "        input: tensor of shape (seq_len, batch, input_size)\n",
    "        hidden: tensor of shape (batch, hidden_size), initial hidden activity\n",
    "            if None, hidden is initialized through self.init_hidden()\n",
    "\n",
    "    Outputs:\n",
    "        output: tensor of shape (seq_len, batch, hidden_size)\n",
    "        hidden: tensor of shape (batch, hidden_size), final hidden activity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, dt=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tau = 100\n",
    "        if dt is None:\n",
    "            alpha = 1\n",
    "        else:\n",
    "            alpha = dt / self.tau\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.input2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def init_hidden(self, input_shape):\n",
    "        batch_size = input_shape[1]\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "\n",
    "    def recurrence(self, input, hidden):\n",
    "        \"\"\"Run network for one time step.\n",
    "\n",
    "        Inputs:\n",
    "            input: tensor of shape (batch, input_size)\n",
    "            hidden: tensor of shape (batch, hidden_size)\n",
    "\n",
    "        Outputs:\n",
    "            h_new: tensor of shape (batch, hidden_size),\n",
    "                network activity at the next time step\n",
    "        \"\"\"\n",
    "        h_new = torch.relu(self.input2h(input) + self.h2h(hidden))\n",
    "\n",
    "        #implement how much the previous hidden layer activity should be maintained in the new activity\n",
    "        h_new = hidden * (1 - self.alpha) + h_new * self.alpha\n",
    "        return h_new\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \"\"\"Propogate input through the network.\"\"\"\n",
    "\n",
    "        # If hidden activity is not provided, initialize it\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(input.shape).to(input.device)\n",
    "\n",
    "        # Loop through time\n",
    "        output = []\n",
    "        steps = range(input.size(0))\n",
    "        for i in steps:\n",
    "            hidden = self.recurrence(input[i], hidden)\n",
    "            output.append(hidden)\n",
    "\n",
    "        # Stack together output from all time steps\n",
    "        output = torch.stack(output, dim=0)  # (seq_len, batch, hidden_size)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class RNNNet(nn.Module):\n",
    "    \"\"\"Recurrent network model.\n",
    "\n",
    "    Parameters:\n",
    "        input_size: int, input size\n",
    "        hidden_size: int, hidden size\n",
    "        output_size: int, output size\n",
    "\n",
    "    Inputs:\n",
    "        x: tensor of shape (Seq Len, Batch, Input size)\n",
    "\n",
    "    Outputs:\n",
    "        out: tensor of shape (Seq Len, Batch, Output size)\n",
    "        rnn_output: tensor of shape (Seq Len, Batch, Hidden size)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Leaky RNN\n",
    "        self.rnn = LeakyRNN(input_size, hidden_size, **kwargs)\n",
    "\n",
    "        # Add a Linear output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_output, _ = self.rnn(x)\n",
    "        out = self.fc(rnn_output)\n",
    "        return out, rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNNet(\n",
      "  (rnn): LeakyRNN(\n",
      "    (input2h): Linear(in_features=3, out_features=128, bias=True)\n",
      "    (h2h): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n",
      "Training network...\n",
      "Step 100, Loss 0.1700, Time 3.0s\n",
      "Step 200, Loss 0.0829, Time 5.9s\n",
      "Step 300, Loss 0.0568, Time 9.0s\n",
      "Step 400, Loss 0.0431, Time 12.0s\n",
      "Step 500, Loss 0.0378, Time 15.0s\n",
      "Step 600, Loss 0.0337, Time 18.1s\n",
      "Step 700, Loss 0.0319, Time 21.2s\n",
      "Step 800, Loss 0.0296, Time 24.4s\n",
      "Step 900, Loss 0.0284, Time 27.5s\n",
      "Step 1000, Loss 0.0285, Time 30.6s\n",
      "Step 1100, Loss 0.0337, Time 33.8s\n",
      "Step 1200, Loss 0.0286, Time 37.0s\n",
      "Step 1300, Loss 0.0274, Time 40.2s\n",
      "Step 1400, Loss 0.0257, Time 43.6s\n",
      "Step 1500, Loss 0.0295, Time 46.8s\n",
      "Step 1600, Loss 0.0262, Time 50.2s\n",
      "Step 1700, Loss 0.0262, Time 53.4s\n",
      "Step 1800, Loss 0.0250, Time 56.7s\n",
      "Step 1900, Loss 0.0255, Time 60.0s\n",
      "Step 2000, Loss 0.0262, Time 63.2s\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network and print information\n",
    "hidden_size = 128\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create an instance of the Class RNNNet\n",
    "net = RNNNet(input_size, hidden_size, output_size, dt=env.dt)\n",
    "print(net)\n",
    "\n",
    "net = train_model(net, dataset, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Brain Inspired - penalties for non brain like cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define penalties\n",
    "def sparsity_penalty(model, beta_sparsity=0.01):\n",
    "    \"\"\"Compute sparsity penalty based on the model's weights.\"\"\"\n",
    "    return beta_sparsity * torch.sum(torch.abs(model.fc.weight))\n",
    "\n",
    "def firing_rate_penalty(output, beta_firing_rate=0.1):\n",
    "    \"\"\"Compute penalty for high firing rates based on the model's output.\"\"\"\n",
    "    return beta_firing_rate * torch.sum(torch.square(output))\n",
    "\n",
    "\n",
    "# define loss function\n",
    "def loss_brain_penalties(output, target, model, \n",
    "                                beta_sparsity=0.05, beta_firing_rate=0.5):\n",
    "    \"\"\"\n",
    "    Compute the total loss with added penalties for sparsity, firing rate, and long-distance connections.\n",
    "    \"\"\"\n",
    "    # Base task-specific loss - same as before\n",
    "    loss = nn.CrossEntropyLoss()(output, target)\n",
    "\n",
    "    # Add penalties\n",
    "    total_loss = loss\n",
    "    total_loss += sparsity_penalty(model, beta_sparsity)\n",
    "    total_loss += firing_rate_penalty(output, beta_firing_rate)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network...\n",
      "Step 100, Loss 1.4102, Time 3.0s\n",
      "Step 200, Loss 0.6318, Time 6.1s\n",
      "Step 300, Loss 0.6044, Time 9.2s\n",
      "Step 400, Loss 0.6031, Time 12.3s\n",
      "Step 500, Loss 0.5947, Time 15.3s\n",
      "Step 600, Loss 0.6326, Time 18.5s\n",
      "Step 700, Loss 0.5851, Time 21.8s\n",
      "Step 800, Loss 0.5684, Time 25.0s\n",
      "Step 900, Loss 0.5649, Time 28.2s\n",
      "Step 1000, Loss 0.5655, Time 31.5s\n",
      "Step 1100, Loss 0.5685, Time 34.8s\n",
      "Step 1200, Loss 0.5654, Time 38.0s\n",
      "Step 1300, Loss 0.5682, Time 41.4s\n",
      "Step 1400, Loss 0.5684, Time 44.8s\n",
      "Step 1500, Loss 0.5655, Time 48.1s\n",
      "Step 1600, Loss 0.5694, Time 51.5s\n",
      "Step 1700, Loss 0.5683, Time 55.0s\n",
      "Step 1800, Loss 0.5645, Time 58.3s\n",
      "Step 1900, Loss 0.5658, Time 61.7s\n",
      "Step 2000, Loss 0.5659, Time 65.1s\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Class RNNNet\n",
    "net = RNNNet(input_size, hidden_size, output_size, dt=env.dt) # same as before\n",
    "\n",
    "net = train_model(\n",
    "    net, \n",
    "    dataset, \n",
    "    lambda output, target, model: loss_brain_penalties(output, target, model, beta_sparsity=0.5, beta_firing_rate=0),\n",
    "    model=net  # pass for penalty calc\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Firing Rate Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network...\n",
      "Step 100, Loss 14.9122, Time 2.9s\n",
      "Step 200, Loss 1.1213, Time 5.7s\n",
      "Step 300, Loss 1.1071, Time 8.6s\n",
      "Step 400, Loss 1.1025, Time 11.5s\n",
      "Step 500, Loss 1.1014, Time 14.5s\n",
      "Step 600, Loss 1.1008, Time 18.7s\n",
      "Step 700, Loss 1.1007, Time 22.1s\n",
      "Step 800, Loss 1.1006, Time 25.5s\n",
      "Step 900, Loss 1.1007, Time 29.1s\n",
      "Step 1000, Loss 1.1002, Time 32.6s\n",
      "Step 1100, Loss 1.1007, Time 36.1s\n",
      "Step 1200, Loss 1.1000, Time 39.5s\n",
      "Step 1300, Loss 1.1004, Time 42.9s\n",
      "Step 1400, Loss 1.1000, Time 46.2s\n",
      "Step 1500, Loss 1.0998, Time 49.6s\n",
      "Step 1600, Loss 1.1000, Time 53.2s\n",
      "Step 1700, Loss 1.0998, Time 56.6s\n",
      "Step 1800, Loss 1.1001, Time 60.0s\n",
      "Step 1900, Loss 1.0993, Time 63.3s\n",
      "Step 2000, Loss 1.0998, Time 66.7s\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Class RNNNet\n",
    "net = RNNNet(input_size, hidden_size, output_size, dt=env.dt) # same as before\n",
    "\n",
    "net = train_model(\n",
    "    net, \n",
    "    dataset, \n",
    "    lambda output, target, model: loss_brain_penalties(output, target, model, beta_sparsity=0, beta_firing_rate=0.5),\n",
    "    model=net  # pass for penalty calc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model EI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EIRecLinear(nn.Module):\n",
    "\n",
    "    r\"\"\"Recurrent E-I Linear transformation.\n",
    "\n",
    "    This module implements a linear transformation with recurrent E-I dynamics,\n",
    "    where part of the units are excitatory and the rest are inhibitory.\n",
    "\n",
    "    Args:\n",
    "        hidden_size: int, the number of units in the layer.\n",
    "        e_prop: float between 0 and 1, the proportion of excitatory units.\n",
    "        bias: bool, if True, adds a learnable bias to the output.\n",
    "    \"\"\"\n",
    "\n",
    "    __constants__ = ['bias', 'hidden_size', 'e_prop']\n",
    "\n",
    "    def __init__(self, hidden_size, e_prop, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.e_prop = e_prop\n",
    "        self.e_size = int(e_prop * hidden_size) # Number of excitatory units\n",
    "        self.i_size = hidden_size - self.e_size # Number of inhibitory units\n",
    "\n",
    "        # Weight matrix for the recurrent connections\n",
    "        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "\n",
    "        # Create a mask to define the E-I interactions\n",
    "        # The mask has ones for E to E/I and negative ones for I to E/I, except the diagonal\n",
    "\n",
    "        #First create a mask to remove the diagonal (matrix size hidden_size*hidden_size)\n",
    "        mask_no_diag = torch.ones((hidden_size, hidden_size)) - torch.eye(hidden_size)\n",
    "        # Define the excitatory an inhibitory units with columns of 1s and -1s (use e_size and i_size)\n",
    "        E_I_unit_list = np.concatenate([np.ones(self.e_size), -np.ones(self.i_size)]).T\n",
    "\n",
    "        mask = torch.tensor(E_I_unit_list, dtype=torch.float32) * mask_no_diag\n",
    "\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "        # Optionally add a bias term\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Initialize weights and biases\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        # Scale the weights for the excitatory neurons\n",
    "        self.weight.data[:, :self.e_size] /= (self.e_size/self.i_size)\n",
    "\n",
    "        # Initialize biases\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def effective_weight(self):\n",
    "        # Apply the mask you have already created to the weights after applying rectification to get the effective weight\n",
    "        # This ensures that weights from excitatory neurons are positive,\n",
    "        # and weights from inhibitory neurons are negative.\n",
    "        eff_W = F.relu(self.weight)*self.mask\n",
    "        return eff_W\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Apply the linear transformation using the effective weights and biases\n",
    "        # The weights used are non-negative due to the absolute value in effective_weight.\n",
    "        return F.linear(input, self.effective_weight(), self.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EIRNN(nn.Module):\n",
    "    \"\"\"E-I RNN.\n",
    "\n",
    "    Reference:\n",
    "        Song, H.F., Yang, G.R. and Wang, X.J., 2016.\n",
    "        Training excitatory-inhibitory recurrent neural networks\n",
    "        for cognitive tasks: a simple and flexible framework.\n",
    "        PLoS computational biology, 12(2).\n",
    "\n",
    "    Args:\n",
    "        input_size: Number of input neurons\n",
    "        hidden_size: Number of hidden neurons\n",
    "\n",
    "    Inputs:\n",
    "        input: (seq_len, batch, input_size)\n",
    "        hidden: (batch, hidden_size)\n",
    "        e_prop: float between 0 and 1, proportion of excitatory neurons\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, dt=None,\n",
    "                 e_prop=0.8, sigma_rec=0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.e_size = int(hidden_size * e_prop)\n",
    "        self.i_size = hidden_size - self.e_size\n",
    "        self.num_layers = 1\n",
    "        self.tau = 100\n",
    "        if dt is None:\n",
    "            alpha = 1\n",
    "        else:\n",
    "            alpha = dt / self.tau\n",
    "        self.alpha = alpha\n",
    "        self.oneminusalpha = 1 - alpha\n",
    "        # Recurrent noise parameter, scaled by the discretization (sqrt(2*alpha)) and noise level (sigma_rec)\n",
    "        # This adds stochasticity to the recurrent dynamics, possibly simulating biological neural variability\n",
    "        self._sigma_rec = np.sqrt(2*alpha) * sigma_rec\n",
    "\n",
    "        self.input2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = EIRecLinear(hidden_size, e_prop=0.8)\n",
    "\n",
    "    def init_hidden(self, input):\n",
    "        batch_size = input.shape[1]\n",
    "        return (torch.zeros(batch_size, self.hidden_size).to(input.device),\n",
    "                torch.zeros(batch_size, self.hidden_size).to(input.device))\n",
    "\n",
    "    def recurrence(self, input, hidden):\n",
    "        \"\"\"Recurrence helper.\"\"\"\n",
    "        state, output = hidden\n",
    "        total_input = self.input2h(input) + self.h2h(output)\n",
    "\n",
    "        state = state * self.oneminusalpha + total_input * self.alpha\n",
    "        state += self._sigma_rec * torch.randn_like(state)\n",
    "        output = torch.relu(state)\n",
    "        return state, output\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \"\"\"Propogate input through the network.\"\"\"\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(input)\n",
    "\n",
    "        output = []\n",
    "        steps = range(input.size(0))\n",
    "        for i in steps:\n",
    "            hidden = self.recurrence(input[i], hidden)\n",
    "            output.append(hidden[1])\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"Recurrent network model.\n",
    "\n",
    "    Args:\n",
    "        input_size: int, input size\n",
    "        hidden_size: int, hidden size\n",
    "        output_size: int, output size\n",
    "        rnn: str, type of RNN, lstm, rnn, ctrnn, or eirnn\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Excitatory-inhibitory RNN\n",
    "        self.rnn = EIRNN(input_size, hidden_size, **kwargs)\n",
    "        self.fc = nn.Linear(self.rnn.e_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_activity, _ = self.rnn(x)\n",
    "        rnn_e = rnn_activity[:, :, :self.rnn.e_size]\n",
    "        out = self.fc(rnn_e)\n",
    "        return out, rnn_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alejo\\AppData\\Local\\Temp\\ipykernel_39544\\3936818147.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mask = torch.tensor(mask, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (rnn): EIRNN(\n",
      "    (input2h): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (h2h): EIRecLinear()\n",
      "  )\n",
      "  (fc): Linear(in_features=40, out_features=3, bias=True)\n",
      ")\n",
      "Training network...\n",
      "Step 100, Loss 0.2854, Time 3.9s\n",
      "Step 200, Loss 0.1045, Time 7.8s\n",
      "Step 300, Loss 0.0595, Time 11.7s\n",
      "Step 400, Loss 0.0480, Time 15.6s\n",
      "Step 500, Loss 0.0405, Time 19.7s\n",
      "Step 600, Loss 0.0359, Time 24.4s\n",
      "Step 700, Loss 0.0337, Time 28.8s\n",
      "Step 800, Loss 0.0349, Time 33.2s\n",
      "Step 900, Loss 0.0308, Time 37.4s\n",
      "Step 1000, Loss 0.0296, Time 41.6s\n",
      "Step 1100, Loss 0.0286, Time 46.1s\n",
      "Step 1200, Loss 0.0285, Time 50.3s\n",
      "Step 1300, Loss 0.0295, Time 54.7s\n",
      "Step 1400, Loss 0.0269, Time 58.9s\n",
      "Step 1500, Loss 0.0278, Time 63.2s\n",
      "Step 1600, Loss 0.0271, Time 67.7s\n",
      "Step 1700, Loss 0.0286, Time 72.1s\n",
      "Step 1800, Loss 0.0275, Time 76.6s\n",
      "Step 1900, Loss 0.0252, Time 81.3s\n",
      "Step 2000, Loss 0.0262, Time 85.9s\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network\n",
    "hidden_size = 50\n",
    "net = Net(input_size=input_size, hidden_size=hidden_size,\n",
    "          output_size=output_size, dt=env.dt, sigma_rec=0.15)\n",
    "print(net)\n",
    "\n",
    "\"\"\"\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_values = []  # List to store loss values\n",
    "running_loss = 0.0\n",
    "print_step = 200\n",
    "for i in range(5000):\n",
    "    inputs, labels = dataset()\n",
    "    inputs = torch.from_numpy(inputs).type(torch.float)\n",
    "    labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
    "\n",
    "    # Zero the gradient buffers\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output, activity = net(inputs)\n",
    "    output = output.view(-1, output_size)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(output, labels)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Update running loss\n",
    "    running_loss += loss.item()\n",
    "    if i % print_step == (print_step - 1):\n",
    "        average_loss = running_loss / print_step\n",
    "        print('Step {}, Loss {:0.4f}'.format(i+1, average_loss))\n",
    "        loss_values.append(average_loss)  # Append average loss here\n",
    "        running_loss = 0.0\n",
    "\n",
    "# Plotting the learning curve\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.plot(loss_values, label='Loss')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = train_model(net, dataset, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
